{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Load data file to train model\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import array as arr\n",
    "\n",
    "data = pd.read_csv(\"export_dataframe_84CVs_SKILL_HF.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "# Nº de palavras\n",
    "words = set(list(data['Word'].values))\n",
    "words.add('PADword')\n",
    "print(\"Número de palavras: {}\".format(len(words)))\n",
    "\n",
    "# Nº de tags\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags)\n",
    "print(\"Número de tags: {}\".format(len(tags)))\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "sent = getter.get_next()\n",
    "\n",
    "# Nº de inputs\n",
    "sentences = getter.sentences\n",
    "print(\"Número de inputs: {}\".format(len(sentences)))\n",
    "\n",
    "# Maior numero de palavras num input\n",
    "largest_sen = max(len(sen) for sen in sentences)\n",
    "print('O maior input tem {} palavras'.format(largest_sen))\n",
    "\n",
    "# Distribuição da quantidade da palavras por input\n",
    "%matplotlib inline\n",
    "plt.hist([len(sen) for sen in sentences], bins= 50)\n",
    "plt.show()\n",
    "\n",
    "# Atribuição de um valor numérico a cada uma das palavras do vocabulário existente (Inputs e Tags)\n",
    "words2index = {w:i for i,w in enumerate(words)}\n",
    "tags2index = {t:i for i,t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Split dos inputs por palavra. Consoante o tamanho máximo dos inputs é acrescentada a palavra PADword para todos os inputs terem a mesma dimensão.\n",
    "# Variável new_X fica com as entradas separadas por palavra e já com padding\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "largest_word = max(len(word) for word in words2index)\n",
    "max_len = largest_sen\n",
    "X = [[w[0]for w in s] for s in sentences]\n",
    "new_X = []\n",
    "for seq in X:\n",
    "    new_seq = []\n",
    "    for i in range(max_len):\n",
    "        try:\n",
    "            new_seq.append(seq[i])\n",
    "        except:\n",
    "            new_seq.append(\"PADword\")\n",
    "    new_X.append(new_seq)\n",
    "\n",
    "# Outputs passam para valores numéricos\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "y = [[tags2index[w[1]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tags2index[\"O\"])\n",
    "\n",
    "# Separação de 2 inputs para testar o modelo depois de treinado\n",
    "aux = len(new_X) - 2\n",
    "\n",
    "X_tr, X_te = new_X[:aux], new_X[aux:]\n",
    "y_tr, y_te = y[:aux], y[aux:]\n",
    "\n",
    "print(len(new_X))\n",
    "print(len(y))\n",
    "print(len(X_tr))\n",
    "print(len(y_tr))\n",
    "print(len(X_te))\n",
    "print(len(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Definição do modelo. Para ver o modelo descomentar linha com #model.summary()\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "batch_size = 2\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_len])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "\n",
    "# Modelo\n",
    "input_text = Input(shape=(max_len,), dtype=tf.string)\n",
    "embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\n",
    "x = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
    "\n",
    "model = Model(input_text, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Separação do conjunto de treino em dados de treino e dados de validação\n",
    "# Separação dos inputs em conjunto de treino (80%) e conjunto de validação (20%)\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "val_per = 0.2\n",
    "\n",
    "x = int((1-val_per) * len(X_tr))\n",
    "\n",
    "if (x % 2) != 0: x -= 1\n",
    "\n",
    "X_tr2, X_val2 = X_tr[:x], X_tr[x:]\n",
    "y_tr2, y_val2 = y_tr[:x], y_tr[x:]\n",
    "y_tr2 = y_tr2.reshape(y_tr2.shape[0], y_tr2.shape[1], 1)\n",
    "y_val2 = y_val2.reshape(y_val2.shape[0], y_val2.shape[1], 1)\n",
    "\n",
    "print(len(X_tr))\n",
    "print(len(y_tr))\n",
    "\n",
    "print(len(X_tr2))\n",
    "print(len(y_tr2))\n",
    "print(len(X_val2))\n",
    "print(len(y_val2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Treinar o modelo\n",
    "# -----------------\n",
    "\n",
    "history = model.fit(np.array(X_tr2), y_tr2, validation_data=(np.array(X_val2), y_val2), batch_size=batch_size, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Previsão com dados de teste, obtenção da precisão e dos resultados finais\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "test_pred = model.predict(np.array(X_te), batch_size=batch_size, verbose=1)\n",
    "\n",
    "idx2tag = {i: w for w, i in tags2index.items()}\n",
    "\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"PADword\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "\n",
    "def test2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            out_i.append(idx2tag[p].replace(\"PADword\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "    \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = test2label(y_te)\n",
    "\n",
    "# Precisão\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))\n",
    "print(\"\\n\")\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "\n",
    "# Resultados\n",
    "file1 = open(\"myfiletest.txt\",\"a\")#append mode\n",
    "largest_word = largest_word + 1\n",
    "print(\"Palavra\" + \" \"*(largest_word-7) + \"Prev - Real\")\n",
    "print(\"=\"*(largest_word+14))\n",
    "file1.write(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)) + \"\\n\")\n",
    "file1.write(classification_report(test_labels, pred_labels) + \"\\n\")\n",
    "file1.write(\"Palavra\" + \" \"*(largest_word-7) + \"Prev ---- Real\" + \"\\n\")\n",
    "file1.write(\"=\"*(largest_word+14) + \"\\n\")\n",
    "space = \" \"\n",
    "i=0\n",
    "for pred in pred_labels:\n",
    "    j=0\n",
    "    for pred2 in pred:\n",
    "        if X_te[i][j] == \"PADword\": \n",
    "            break\n",
    "        print(X_te[i][j] + \" \"*(largest_word-len(X_te[i][j])) + pred2 + \" \"*(10-len(pred2)) + test_labels[i][j])\n",
    "        file1.write(X_te[i][j] + \" \"*(largest_word-len(X_te[i][j])) + pred2 + \" \"*(10-len(pred2)) + test_labels[i][j] + \"\\n\")\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Gravar o modelo em formato .pb\n",
    "# -------------------------------\n",
    "\n",
    "#export_path = 'so-test/'\n",
    "\n",
    "#with tf.keras.backend.get_session() as sess:\n",
    "#    tf.saved_model.simple_save(sess, export_path, inputs={'input_image': model.input}, outputs={t.name: t for t in model.outputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tf1': conda)",
   "language": "python",
   "name": "python361064bittf1conda3e686c2688b8465cb871042a2b742222"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}